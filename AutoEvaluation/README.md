# 🧠 AutoEvaluation

AutoEvaluation is a framework for automatically verifying the correctness of Lean 4 theorem statements generated by large language models (LLMs).

It automates the process of compiling Lean files, capturing syntax and semantic errors, and mapping them back to individual theorems for detailed evaluation.


## 🚀 Features
- 🧩 Batch verification of Lean 4 files
- ⚙️ Automatic compilation using Lean CLI or lake build
- 🧾 Error mapping per theorem (links each failure to its Lean statement)
- 🧮 Supports Mathlib and other Lean dependencies
- 📊 Structured output for integration with evaluation pipelines or LLM fine-tuning
- 🔍 Language-model compatibility: designed for datasets from models like Mathesis, ATLAS, or DeepSeek-Coder-33B


## 📂 Project Structure

```
AutoEvaluation/
│
├── AutoEvaluation/
│   ├── Basic.lean              # Core examples and evaluation templates
│   ├── Generated/
│   │   ├── _Header.lean        # Shared imports and open statements
│   │   └── Example.lean        # Model-generated Lean theorems
│   └── tools/
│       └── check_lean_files.py # Python script for automated compilation
│
├── lakefile.toml               # Lean 4 project configuration
├── lean-toolchain              # Specifies Lean version
├── README.md                   # You are here!
└── .gitignore                  # Ignore build artifacts and secrets
```

## ⚙️ Setup

### 1. Prerequisites

Ensure you have:
- Lean 4 (≥ 4.22.0) installed via elan
- Lake (Lean’s build system)
- Python 3.9+ with subprocess and json (default in modern Python)

### 2. Clone and build

```bash
git clone https://github.com/A2DR1/LeanQC_AI.git
cd LeanQC_AI/AutoEvaluation
lake update
lake build
```

### 3. Verify installation

Run a quick check to ensure everything builds:

```bash
lake build AutoEvaluation
```


## 🧪 How to Run Auto Evaluation

### 1. Generate or add Lean theorems

Place your Lean 4 generated outputs under:

```
AutoEvaluation/Generated/Example.lean 
```

Each theorem should follow this format:

```lean
import Generated._Header

/-- Example: Binomial theorem -/
theorem P1_binomial (x : ℝ) :
    (x + 1) ^ 2 = x ^ 2 + 2 * x + 1 := by
  ring
```

### 2. Run the checker

```bash
python tools/EvalScript_byTheorem.py
```

This will:
- Compile every .lean file in the /Generated directory
- Record any syntax/type errors
- Map each error to the specific theorem (e.g., P7_odd_sq → unknown constant 'Odd')
- Print and save the structured results


## 🧾 Example Output

```
🔍 Checking generated Lean files...

▶ Checking Generated/Example.lean...
❌ Failed: theorem P7_odd_sq
Error: unknown constant 'Odd'

✅ Success: theorem P1_binomial
✅ Success: theorem P2_sq_nonneg

📊 Summary:
  2/3 theorems compiled successfully.
  Errors logged to results/eval_log.json
```

## 🧠 Typical Workflow

| Step | Action | Description |
|:----:|:--------|:-------------|
| 1️⃣ | **Generate** | Use your autoformalization model (e.g., *Mathesis*) to produce NL→Lean pairs |
| 2️⃣ | **Store** | Save outputs to `AutoEvaluation/Generated/*.lean` |
| 3️⃣ | **Run** | Execute `check_lean_files.py` to batch-compile |
| 4️⃣ | **Analyze** | View results per theorem (JSON log) |
| 5️⃣ | **Iterate** | Retrain or refine prompts/models based on error distribution |


## 🧩 Configuration

Modify lakefile.toml to include dependencies (e.g., mathlib):

```toml
[[lean_lib]]
name = "Generated"

[[require]]
name = "mathlib"
scope = "leanprover-community"
```

Edit Generated/_Header.lean to define imports shared by all generated files:

```lean
import Mathlib
open scoped BigOperators
open Real Nat Topology Filter
```


## 📈 Future Extensions

- 🔍 Fine-grained error tagging by theorem + line number
- 🤖 LLM feedback integration (auto-repairing incorrect theorems)
- 📊 Evaluation dashboard for model accuracy tracking
- 🧬 Cross-model benchmarking (Mathesis, ATLAS, KELPS, etc.)

## 🧑‍💻 Author

**Austin Shen**

University of Michigan

Research focus: Automated theorem verification, LLM-driven formal reasoning, and hybrid symbolic–neural AI.