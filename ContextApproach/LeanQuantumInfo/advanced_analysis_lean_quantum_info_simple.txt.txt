ADVANCED TOKEN ANALYSIS REPORT
==================================================
File: lean_quantum_info_simple.txt
Path: /Users/austinshen/Documents/Umich/Research/LeanQC_AI/ContextApproach/LeanQuantumInfo/lean_quantum_info_simple.txt

FILE STATISTICS:
  Size: 629,166 bytes
  Characters: 600,055
  Words: 95,193
  Lines: 14,508
  Non-empty lines: 12,109

TOKEN ESTIMATES BY MODEL:
  gpt-3.5-turbo       :  172,132 tokens (4202.4% of limit) ✗ TOO LARGE
  gpt-4               :  172,132 tokens (2101.2% of limit) ✗ TOO LARGE
  gpt-4-turbo         :  172,132 tokens (134.5% of limit) ✗ TOO LARGE
  gpt-4o              :  172,132 tokens (134.5% of limit) ✗ TOO LARGE
  claude-3-sonnet     :  172,132 tokens ( 86.1% of limit) ✓ FITS
  claude-3-opus       :  172,132 tokens ( 86.1% of limit) ✓ FITS
  gemini-1.5-pro      :  172,132 tokens (  8.6% of limit) ✓ FITS
  gemini-1.5-flash    :  172,132 tokens ( 17.2% of limit) ✓ FITS

CHUNKING RECOMMENDATIONS:
  gpt-3.5-turbo: Needs chunking (172,132 tokens > 4,096 limit)
    Recommended chunk size: 3,096 tokens
  gpt-4: Needs chunking (172,132 tokens > 8,192 limit)
    Recommended chunk size: 7,192 tokens
  gpt-4-turbo: Needs chunking (172,132 tokens > 128,000 limit)
    Recommended chunk size: 127,000 tokens
  gpt-4o: Needs chunking (172,132 tokens > 128,000 limit)
    Recommended chunk size: 127,000 tokens
  claude-3-sonnet: No chunking needed
  claude-3-opus: No chunking needed
  gemini-1.5-pro: No chunking needed
  gemini-1.5-flash: No chunking needed

DETAILED CHUNKING STRATEGY FOR GPT-3.5-TURBO:
  Total tokens: 172,132
  Context limit: 4,096
  Excess tokens: 168,036

  Available strategies:
    1. Split into chunks of ~260 lines each
       Estimated chunks: 56
       Lines per chunk: 260

    2. Split into chunks of ~10,792 characters each
       Estimated chunks: 56
       Characters per chunk: 10,792
